{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Source: https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Goal:\n",
    "\n",
    "The open data provided by NYC TLC as csv files for each months from year 2009 onwards containing millions/billions of rows. Here each row corresponds to a single ride. Working with such huge data is impossible in data visualization tools like tableau and PowerBI. The below code aggrgates the number of trips by date and pickup and drop-off location. The code returns one file each for pickup and drop-off for a year."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Declare static variables (csv files, etc) and import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb\n",
    "import pandas as pd\n",
    "import multiprocessing\n",
    "from multiprocessing import Pool, Lock, Process, Manager\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "from pandas import read_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoVivification(dict):\n",
    "    \"\"\"Implementation of perl's autovivification feature.\"\"\"\n",
    "    def __getitem__(self, item):\n",
    "        try:\n",
    "            return dict.__getitem__(self, item)\n",
    "        except KeyError:\n",
    "            value = self[item] = type(self)()\n",
    "            return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_dict(window,total):\n",
    "        if bool(dictionary[window]) == True:\n",
    "                value = dictionary[window]\n",
    "                value = float(value) + float(total)\n",
    "                dictionary[window] = value\n",
    "        if bool(dictionary[window]) == False:\n",
    "                dictionary[window] = total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_dict2(ID,window,total):\n",
    "        if bool(dictionary2[ID][window]) == True:\n",
    "                value = dictionary2[ID][window]\n",
    "                value = float(value) + float(total)\n",
    "                dictionary2[ID][window] = value\n",
    "        if bool(dictionary2[ID][window]) == False:\n",
    "                dictionary2[ID][window] = total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_dict3(ID,window,total):\n",
    "        if bool(dictionary3[ID][window]) == True:\n",
    "                value = dictionary3[ID][window]\n",
    "                value = float(value) + float(total)\n",
    "                dictionary3[ID][window] = value\n",
    "        if bool(dictionary3[ID][window]) == False:\n",
    "                dictionary3[ID][window] = total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Extract relevant information from csv file and store them in a Data Frame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_info(file_1):\n",
    "    \"\"\"\n",
    "    param: file.csv\n",
    "    return: dataframe\n",
    "    This function takes as input a file and stores usefule information into a dataframe\n",
    "    \"\"\"\n",
    "    data = pd.read_csv(file_1)\n",
    "    data = data[['VendorID','tpep_pickup_datetime','tpep_dropoff_datetime','PULocationID','DOLocationID','passenger_count','trip_distance','total_amount']].copy()\n",
    "    return data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Data files for yellow taxi cab (2019)\n",
    "\"\"\"\n",
    "\n",
    "taxi_01_2019 = extract_info('yellow_tripdata_2019-01.csv')\n",
    "taxi_02_2019 = extract_info('yellow_tripdata_2019-02.csv')\n",
    "taxi_03_2019 = extract_info('yellow_tripdata_2019-03.csv')\n",
    "taxi_04_2019 = extract_info('yellow_tripdata_2019-04.csv')\n",
    "taxi_05_2019 = extract_info('yellow_tripdata_2019-05.csv')\n",
    "taxi_06_2019 = extract_info('yellow_tripdata_2019-06.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Concatenate taxi cab information from csv files, starting with data from 2015 up until 2019 and store them into DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = [taxi_01_2019,taxi_02_2019,taxi_03_2019,taxi_04_2019,taxi_05_2019,taxi_06_2019] #information for 2019\n",
    "taxi_2019 = pd.concat(n,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44459136\n"
     ]
    }
   ],
   "source": [
    "dictionary = AutoVivification()\n",
    "print(len(taxi_2019['tpep_pickup_datetime']))\n",
    "for index,value in enumerate(taxi_2019['tpep_pickup_datetime']):\n",
    "    add_dict(taxi_2019['tpep_pickup_datetime'][index].split()[0],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('taxi_2019.csv', 'w') as f:\n",
    "    for key in dictionary.keys():\n",
    "        f.write(\"%s,%s\\n\"%(key,dictionary[key]))\n",
    "\n",
    "x = read_csv('taxi_2019.csv')\n",
    "x.columns = ['Date','Number of Rides']\n",
    "x.to_csv('taxi_2019.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heap_map(taxi,location):\n",
    "    \"\"\"\n",
    "    param: file.csv\n",
    "    return: dataframe\n",
    "    This function takes as input a dataframe and compares features of dataframe to find correlation in data\n",
    "    \"\"\"\n",
    "    taxi_loc = pd.read_csv(location)\n",
    "    \n",
    "    data_loc_and_bor = taxi_loc[['LocationID','Borough']]\n",
    "    \n",
    "    data_loc_and_bor.columns = ['PULocationID','PUBorough']\n",
    "    taxi1 = taxi[['PULocationID','tpep_pickup_datetime']]\n",
    "    taxi1.columns = ['PULocationID','tpep_pickup_datetime']\n",
    "    taxi1 = data_loc_and_bor.merge(taxi1, on=['PULocationID'], how='right',sort=False) \n",
    "    \n",
    "    data_loc_and_bor.columns = ['DOLocationID','DOBorough']\n",
    "    taxi2 = taxi[['DOLocationID','tpep_dropoff_datetime']]\n",
    "    taxi2.columns = ['DOLocationID','tpep_dropoff_datetime']  \n",
    "    taxi2 = data_loc_and_bor.merge(taxi2, on=['DOLocationID'], how='right',sort=False) \n",
    "    \n",
    "    taxi = pd.concat([taxi1,taxi2],axis = 1,sort=False)\n",
    "    return taxi\n",
    "    #use dataframe to find correlation in data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "taxi_01_2019_heap = heap_map(taxi_01_2019,'../taxi+_zone_lookup.csv')\n",
    "taxi_02_2019_heap = heap_map(taxi_02_2019,'../taxi+_zone_lookup.csv')\n",
    "taxi_03_2019_heap = heap_map(taxi_03_2019,'../taxi+_zone_lookup.csv')\n",
    "taxi_04_2019_heap = heap_map(taxi_04_2019,'../taxi+_zone_lookup.csv')\n",
    "taxi_05_2019_heap = heap_map(taxi_05_2019,'../taxi+_zone_lookup.csv')\n",
    "taxi_06_2019_heap = heap_map(taxi_06_2019,'../taxi+_zone_lookup.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = [taxi_01_2019_heap,taxi_02_2019_heap,taxi_03_2019_heap,taxi_04_2019_heap,taxi_05_2019_heap,taxi_06_2019_heap] #information for 2019\n",
    "taxi_2019 = pd.concat(n,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44459136\n"
     ]
    }
   ],
   "source": [
    "dictionary2 = AutoVivification()\n",
    "dictionary3 = AutoVivification()\n",
    "print(len(taxi_2019['tpep_pickup_datetime']))\n",
    "\n",
    "for index,value in enumerate(taxi_2019['tpep_pickup_datetime']):\n",
    "    add_dict2(taxi_2019['PULocationID'][index],taxi_2019['tpep_pickup_datetime'][index].split()[0],1)\n",
    "\n",
    "for index,value in enumerate(taxi_2019['tpep_dropoff_datetime']) :\n",
    "    add_dict3(taxi_2019['PULocationID'][index],taxi_2019['tpep_dropoff_datetime'][index].split()[0],1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('taxi_heap_2019_pick.csv', 'w') as f:\n",
    "    for key,v1 in dictionary2.items():\n",
    "        key1 = key\n",
    "        key2 = list(v1.items())[0][0]\n",
    "        val = list(v1.items())[0][1]\n",
    "        f.write(\"%s,%s,%s\\n\"%(key1,key2,val))\n",
    "        \n",
    "with open('taxi_heap_2019_drop.csv', 'w') as f:\n",
    "    for key,v1 in dictionary3.items():\n",
    "        key1 = key\n",
    "        key2 = list(v1.items())[0][0]\n",
    "        val = list(v1.items())[0][1]\n",
    "        f.write(\"%s,%s,%s\\n\"%(key1,key2,val))\n",
    "        \n",
    "x = read_csv('taxi_heap_2019_pick.csv')\n",
    "y = read_csv('taxi_heap_2019_drop.csv')\n",
    "x.columns = ['PULocationID','Date','Number of Rides']\n",
    "y.columns = ['DOLocationID','Date','Number of Rides']\n",
    "x.to_csv('taxi_heap_2019_pick.csv')\n",
    "y.to_csv('taxi_heap_2019_drop.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndictionary2 = AutoVivification()\\ndictionary3 = AutoVivification()\\nprint(len(taxi_2019['PULocationID']))\\n\\nfor index,value in enumerate(taxi_2019['tpep_pickup_datetime']['PUBorough']):\\n    add_dict(dictionary2,taxi_2019['tpep_pickup_datetime']['PUBorough'][index].split()[0],1)\\n    \\nfor index,value in enumerate(taxi_2019['DOBorough']):\\n    add_dict(dictionary3,taxi_2019['DOBorough'][index].split()[0],1)\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "dictionary2 = AutoVivification()\n",
    "dictionary3 = AutoVivification()\n",
    "print(len(taxi_2019['PULocationID']))\n",
    "\n",
    "for index,value in enumerate(taxi_2019['tpep_pickup_datetime']['PUBorough']):\n",
    "    add_dict(dictionary2,taxi_2019['tpep_pickup_datetime']['PUBorough'][index].split()[0],1)\n",
    "    \n",
    "for index,value in enumerate(taxi_2019['DOBorough']):\n",
    "    add_dict(dictionary3,taxi_2019['DOBorough'][index].split()[0],1)\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
